## Deep learning interview
- Activation function을 왜 쓰는가?
    - 그냥 쌓아버리면 단순히 W_1 W_2 X의 새로운 linear function이다
    - activation function을 통해 non linear하게 학습이 가능하다
    - 이런식으로 학습하면 Universal approximation theorem 가능.
        - ReLU를 사용함으로써, gradient vanishing 문제를 해결
- Data가 적을 때는?
- Cross validation
- Cross entropy 관련?
- bagging, boosting 차이?


