## [NLP trend](https://kakaobrain.com/blog/118?fbclid=IwAR3s3h0wLIvjV_K9h9ZXsnczts-T50LqU8zXHpq0LWf6rTVPMz8tuksGKag) from kakao brain 
---
- NLP
    - 자연어 이해 (Natural Language Understaing, NLU)
        - 자연어 형태의 문장을 **이해** 시키는 기술
            - 감정 분석, 기계 독해, 상식 추론, 의미론적 유사도 측정, 목적 기반 대화, 관계 추출, 의미론적 구문 분석, 요약
    - 자연어 생성 (Natural Language Generation, NLG)
        - 자연어 문장을 **생성**하는 기술
            - 자동완성, 스토리 생성, 생성형 언어 모델, 데이터 기반 문장 생성, 캡션 생성
        - * 공통 부분: 생성형 문서 요약, 생성형 질의 응답, E2E 챗봇
- history
    - Statistical machine translation(SMT) -> rule-based machine translation(RBMT)
    - 2014년 이후 Neural machine translation(NMT)인 seq2seq을 기반으로 attention mechanism이 제안됨
    - word2vec(2013) 등의 단어 임베딩을 통해 단어(token)을 연속적인 벡터로써 나타냄, 직접적인 해석은 어려워졌지만 대신 모호성과 유의성 문제를 잘해결+end-to-end모델
        - 신경망을 사용하여 효과적으로 단어들을 latant space에 성공적으로 투사시킴
    - 딥러닝을 통한 형태소 분석, 문자 파싱, 개체명 인식, 의미역 결정등에서도 최고 성능을 이뤄냄
    - RNN의 단점을 보완한 LSTM과 GRU의 활용법이 고도화
    - attention으로 긴 길이(time-step)의 시퀀셜 데이터에 대해서도 훈련이 쉽게 가능
    - 자연어 생성 Narual Language Generation(NLG)
    - 메모리를 활용한 연구 Memory Augmented Nueral Network(MANN) : 우리가 원하는 정보를 신경망을 통해 저장하고 필요할 때 잘 조합하여 꺼내쓰는 QA와 같은 문제에 효율적으로 대응할 수 있다.
    - 강화학습을 활용하여 seqGAN과 같이 GAN을 구현하는 방법을 제시하기도 함
- challenging
    - 모호성
    - 중의성(context를 해결해야함)
    - 다양한 표현
    - 불연속적인 데이터
        - curse of dimension, sparseness를 해결하기위해 단어를 적절해 segmentation하고 dimendion reduction 수행
        - 노이즈와 정규화: 뜻이 달라져 버리는 경우가 생긴다.
    - 한국어 전처리
        - 교착어: 어순이 중요시되는 영어나 중국어와 달리 어근에 접사가 붙어 의미와 문법적 기능이 부여됨
        - 띄어쓰기: 추가적인 띄어쓰기 정제 과정 필요
        - 평서문과 의문문
        - 주어 생략
        - 한자 기반의 언어
---
#### NLU
- '이해한다' : 현재 기술 수준에 근거해 주어진 과제에 적절한 답을 내놓는 기계
    - 기계의 이해 능력이 좋아져야 사용자가 만족할 만한 수준의 정보나 답을 들을 수 있다.
    - 라벨링 데이터를 확보하는 일 자체가 난관
        - 많은 양의 텍스트 데이터를 모으기 어렵다
        - 여러 과제를 사전학습하는 데 유용한 라벨링 방식조차 논의되기 어렵다.
            - $\rightarrow$ 비라벨링 데이터만 주어진 상태에서 입력 데이터 일부를 라벨로 사용하거나 사전지식에 따라 라벨을 만들어 모델을 훈련하는 self-supervised learning
            - 텍스트 훈련에 효과적인 딥러닝 모델 아키텍처 개발
            - GPU와 딥러닝 라이브러리 발전
                - 효과적인 사전 훈련하는 모델 개발이 가능해짐
---
#### 평가?
- 언어를 이해한다는 해우이는 구체적으로 **정의**도 **검증**도 어렵다.
    - Sentiment Analysis: SST, IMDb, NSMC
    - Similarlity Prediction: STS, MRPC, QQP, PAWS-X
    - Natual Language Inference: SNLI, MNLI, XNLI, aNLI
    - Linguistic Acceptability: CoLA
    - Reading Comprehension : SQuaAD, RACE, MS MARCO, KorQuAD
    - Intent Classification : ATIS, SNIPS, AskUbuntu
        - 우선은 이걸 중심으로 하는데 GCN도 같이 할 수 있는 지 보기
    - 언어 모델의 성능 평가: GLUE, SuperGLUE라는 벤치 마크.
---
- ELMo(2018, Allen AI)
    - 양방향 LSTM 아키텍처를 이용해 주어진 sequence 다음에 오는 단어(순방향) 또는 앞에 오는 단어(역방향)을 예측한다.
    - Word2Vec은 문맥에 따라 다른 의미를 가지는 다의어를 하나의 방식으로만 표현
        - 다의어를 문맥에 다르게 표현할 수 있다. 2~5개의 단어만 고려하는 Word2Vec과 달리, 128~1024 단어를 고려할 수 있음.
    - 단점: 문장이 길수록 계산 속도가 느려지고 거리가 먼 단어 간 관계를 제대로 표현하기 어려움.
        - 이를 극복하기 위해 Transformer 구조를 활용
- GPT-1(2018, OpenAI)
    - Transformer를 이용한 순방향 언어 모델
    - feature vector를 다시 단어로 복원하는 디코더 구조상 문장을 수월하게 생성할 수 있음.
        - GPT-2는 모델의 규모를 키워 문장 생성 과제에 집중
- BERT(2018, Google)
    - Transformer의 인코더만 이용한 완전 양방향 사전 훈련 언어 모델
    - 단어 시퀀스의 복잡한 관계를 잘 표현할 수 있도록 학습 문장마다 매번 서로 다르게 무작위로 비운 단어를 예측할 수 있가 함(masked language model, MLM)
        - 각 문장을 구성하는 단어 중 15%를 [Mask] 토큰으로 변경하는 식
    - 서로 인접한 두 문장 간의 관계를 예측하는 방법(next sentence prediction, NSP)도 학습.
        - 두개의 문장 중 두번째 문장이 첫번째 문장의 다음 문장인지를 맞출 수 있도록 관련성이 있는 두 문장(positive example)과 관련성이 없는 문장(negative example)dmf 1:1의 비율로 학습 데이터셋을 구성
            - 주어진 문장의 전후 문장을 함께 학습하면 문장을 유용한 임베딩 벡터로 변환하는 연구 접근 방식과 유사함
        - 단점: 여러 단어를 동시에 예측할 때 해당 단어 간 상관관계를 고려하지 않는 점
- XLNet(2019, CMU, Google Brain)
    - BERT의 문제를 해결하기 위해 앞뒤 문맥을 동시에 고려하는 양방향 언어 모델과 예측 단어 간 상관관계를 고려하는 순방향 모델의 장점을 합침
    - 단점: 모델이 지나치게 복잡, 상대적으로 덜 사용
- RoBERTa(2019, FAIR)
    - BERT보다 성능을 한 단계 업그레이드 한 버전
        - 데이터양 13GB->160GB
        - 학습 횟수 125,000->500,000회
        - 배치 크기 256->8,192
        - 사전 크기 32,000->50,000
- ALBERT(2019, Google, A Littel BERT)
    - BERT보다 가벼운 모델
        - 매개변수 수를 줄여 같은 구조의 모델에서의 메모리 사용량을 줄이고 학습 속도를 높임
        - 각 단어의 저차원의 임베딩 벡터로 먼저 표현하고 나서 이를 다시 모델 은닉층의 차원 수만큼 확장
        - Transformer 인코더 블록 간 매개변수를 공유
        - 매개변수수는 1/18, 학습 속도 1.7배 상승
- T5 (2019, Google)
    - T5는 한 모델로 모든 문제를 풀면서도 모델의 규모를 키우겠다는 사고의 집약체 
    - 변수수(110억개), 데이터(700GB)
---
- 한계점
    - GLUE나 SuperGLUE와 같은 최신 벤치마크에서 최고 성능을 달성한 모델이 실제로 자연어를 이해했다고 보기가 어렵다는 정황은 여러 NLU 과제에서 발견되고 있다.
---
- 2020년 NLU 트렌드?
    - relation-based question-answering, knowledge graph를 이용해 사실관계(entity relations)를 표현하려는 시도가 있었음.
    - inferential knowledge graph + GPT 추론 능력을 높이려는 시도
    - MobileBERT나 sentenceBERT처럼 검색에 필요한 임베딩 방법론을 연구하는 트렌드
    - BERT를 활용 하는 것도 
        - 어떤 데이터를 얼마나 사용해야 하는지
        - 어떻게 하면 모델을 효과적으로 훈련할 수 있을지
        - 어떻게 해야 편향과 아티팩트에도 흔들리지 않는 견고한 모델을 만들 수 있을 지
---
#### 전처리
- corpus: 여러 단어들로 이루어진 '문장'
- 개요
    - 1. 코퍼스 수집
    - 2. 정제 (normalization)
        - 원하는 업무와 문제에 따라 또는 응용 분야에 따라 필요한 정제의 수준이나 깊이가 다르다.
        - 2.1. 전각 문자 제거
        - 2.2. 대소문자 통일: 일부 영어 코퍼스에는 약자 등에서 대소문자 표현이 통일되지 않을 떄가 있다. 이러한 표현들을 일원화하여 하나의 의미를 지니는 여러 단어를 하나의 형태로 통일해 sparsity를 줄이는 효과 기대. 하지만, 임베딩으로 해결 가능
        - 2.3. 정규식을 사용한 정제. 노이즈를 효율적으로 감지하고 없애려면 인덱스의 사용이 필수[사이트]('https://regexper.com/')
    - 3. 문장 단위 분절
    - 4. 분절
    - 5. 병렬 코퍼스 정령(optional)
    - 6. 서브 워드 분절
---
#### 단어 임베딩
- 개념
    - 코퍼스로부터 단어 feature를 추출하여 벡터로 만든다.
    - 임베딩은 단어를 벡터로 표현하는 방법임, 해당 임베딩의 목적식과 텍스트 분류의 목적식은 다르다.
    - 임베딩 레이어를 제공한다. 해당 벡터와 원핫 벡터를 곱하면 임베딩 벡터 차원이 나온다(나머지는 다 0이 되므로)
        - 실제로는 lookup으로 찾아서 수행함
- 차원 축소: sparse 문제
    - PCA: 고차원의 데이터를 더 낮은 차원으로 효과적으로 압축
    - manifold hythothesis
    - auto encoder
        - latant space를 구한다.
- word2vec
    - 가정: 함께 등장하는 단어가 비슷할수록 비슷한 벡터 값을 다진다.
    - Continuous bag og words (CBOW)
        - 신경망은 주변에 나타나는 단어들을 원핫 인코딩된 벡터로 입력받아 해당 단어를 예측하게 한다.
    - Skip-gram
        - 학습할 때는 대상 단어를 원핫 인코딩된 벡터로 받아 주변에 나타나는 단어를 예측하는 네트워크를 구성해 단어 임베딩
    - 보통 Skip-gram이 CBOW보다 성능이 뛰어나다고 알려져있음.
- Global vectors for word representation (GloVe)
    - 대상 단어에 대해서 코퍼스와 함께 나타난 단어별 출현 빈도를 예측함.
    - MSE로 회귀문제다
    - 장점: 코퍼스를 통해 단어별 동시 출현 빈도를 조사하여 그에 대한 출현 빈도 행렬을 만들고, 이후엔 해당 행렬을 통해 동시 출현 빈도를 근사한다. skip-gram보다 훨씬 빠른 장점이 있다. skip-gram에서는 출현 빈도가 적은 단어들은 비교적 부정확한 단어 임베딩 벡터를 학습하지만, Glove는 skip-gram에 비해 어느정도 단점을 정리함.
---
#### 정규식 정리

